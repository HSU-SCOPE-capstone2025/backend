import os
import pandas as pd
from datetime import datetime, timedelta

# íƒ€ê²Ÿ ì¸í”Œë£¨ì–¸ì„œ ì±„ë„ ID
target_channels = {
    'UCEX1cZB5TL7jyKejXdTXCKA': 'ë„ë„',
    'UCYJDUekoQz0-bo8al1diLWQ': 'ë§ì™•',
    'UC9ta639M37zzWKwo7kKc80A': 'ì£¼ë‘¥ì´ë°©ì†¡',
    'UCUj6rrhMTR9pipbAWBAMvUQ': 'ì¹¨ì°©ë§¨',
    'UCTx3aCntDvkq-hGtOjKVSnQ': 'ë¨¹ì–´ë³¼ë˜ TryToEat',
    'UCRWxH4fGhuNsh0klWnDbt0w': 'ì–¸ë”ì›”ë“œ',
    'UCA6KBBX8cLwYZNepxlE_7SA': 'íˆë°¥',
    'UCQr8QXrzpIE1yCz7l283rqQ': 'ì§€í•œ',
    'UC98TOxKQk4aLcx0EjIK0LkQ': 'í•´ìˆ˜ì¸',
    'UCvakL6TODG0Wm1ZFN3y13AA': 'ë°•ì—ìŠ¤ë”',
    'UCC9pQY_uaBSa0WOpMNJHbEQ': 'ìì·¨ìš”ë¦¬ì‹ ',
    'UCTQGAYPtbnCEfW9IGx65kiw': 'ë¯¼ìŠ¤ì½”',
    'UCBkyj16n2snkRg1BAzpovXQ': 'ìš°ì™êµ³',
    'UC0VR2v4TZeGcOrZHnmwbU_Q': 'ìœ¡ì‹ë§¨',
    'UCmJEpV4hLzGWLU5rrdOHMhQ': 'ë”ë“¤ë¦¬',
    'UChU7a6tVcJ-PEG4VW0dL36w': 'ë»”ë”',
    'UC_BVJYxQxN5jberGxAFDlyg': 'ì€ìˆ˜ì €',
    'UC1OU9eIk4JGnh9DN1yxwW9A': 'ì¸ìƒ ë…¹ìŒ ì¤‘',
    'UCyozK5OFN5lDrwim5wqQnLA': 'ì·¨ë¯¸ë¡œ ìš”ë¦¬í•˜ëŠ” ë‚¨ì',
    'UCCb6W2FU1L7j9mw14YK-9yg': 'ê¹€ë©”ì£¼ì™€ ê³ ì–‘ì´ë“¤',
    'UCyar0OYt0LoPzkkWcQAo6OA': 'ì „ì–¸ë‹ˆ',
    'UCRXL4vnST2AE6UtjrYMv4tw': 'ë½€ìš©ë‡½',
    'UCkEWsTetbUuGDg_O1S5UVBg': 'ì‚´ë¦¼ë‚¨',
    'UCwiSPfQKmSaOJDZrXN_AyAA': 'ìš”ë¦¬í•˜ëŠ” ë¯¼ì‚¬ì¥',
    'UCj-durTg1W7uWsB8oq0u7kA': 'ì—”ì¡°ì´ì»¤í”Œ',
    'UC8CIM3d3zDYMk-3T5aAz0yw': 'í˜œì•ˆ',
    'UCaHGOzOyeYzLQeKsVkfLEGA': 'ì§€ë¬´ë¹„',
    'UCMz9HhOzc6yPcz0HQF6xVtQ': 'ë‹¬ë‹¬íˆ¬ì–´',
    'UCPvwqht-XvcbbaUavs53ejg': 'ì…ì‹œë•í›„',
    'UCW-QBEmxsme2FAONSnTjp-g': 'ë¼ë¼ baby_pig_rabbit',
}

# ë‚ ì§œ ë²”ìœ„
start_date = datetime.strptime("2025-02-23", "%Y-%m-%d")
end_date = datetime.strptime("2025-05-06", "%Y-%m-%d")

# íŒŒì¼ ê²½ë¡œ
STATS_DIR = '/Users/syb/Desktop/youtube_crawling/data_stats'
COMMENTS_DIR = '/Users/syb/Desktop/youtube_crawling/data_crawling_auto'
SAVE_ROOT = '/Users/syb/Desktop/data_merge'

def load_stats():
    all_stats = []
    for single_date in pd.date_range(start_date, end_date):
        suffix = single_date.strftime("%y%m%d")
        path = os.path.join(STATS_DIR, f"stats_{suffix}.xlsx")
        if os.path.exists(path):
            df = pd.read_excel(path)
            all_stats.append(df)
    if all_stats:
        return pd.concat(all_stats, ignore_index=True)
    return pd.DataFrame()

def load_comments_per_week():
    comments_by_week = {}
    current = start_date
    while current <= end_date:
        week_start = current
        week_end = min(current + timedelta(days=6), end_date)
        temp = []
        for single_date in pd.date_range(week_start, week_end):
            suffix = single_date.strftime("%y%m%d")
            path = os.path.join(COMMENTS_DIR, f"crawling_auto_{suffix}.xlsx")
            if os.path.exists(path):
                df = pd.read_excel(path)
                temp.append(df)
        if temp:
            comments_by_week[(week_start, week_end)] = pd.concat(temp, ignore_index=True)
        current += timedelta(days=7)
    return comments_by_week

def save_data():
    stats_df = load_stats()
    comments_weeks = load_comments_per_week()

    for channel_id, name in target_channels.items():
        save_path = os.path.join(SAVE_ROOT, name)
        os.makedirs(save_path, exist_ok=True)

        # ğŸ“Š ì˜ìƒ ì •ë³´ ë³‘í•© ë° ìµœì‹  ì •ë³´ë¡œ ì •ì œ
        stats_filtered = stats_df[stats_df['channel_id'] == channel_id]
        stats_latest = stats_filtered.sort_values('date').drop_duplicates('video_id', keep='last')
        stats_latest.to_excel(os.path.join(save_path, 'video_stats.xlsx'), index=False)

        # ğŸ’¬ ëŒ“ê¸€ ì£¼ì°¨ë³„ ë³‘í•© ë° ì €ì¥
        for (start, end), comments_df in comments_weeks.items():
            sub_df = comments_df[comments_df['video_id'].isin(stats_latest['video_id'])]
            if not sub_df.empty:
                week_file = f"comments_{start.strftime('%y%m%d')}_{end.strftime('%y%m%d')}.xlsx"
                sub_df.to_excel(os.path.join(save_path, week_file), index=False)

save_data()